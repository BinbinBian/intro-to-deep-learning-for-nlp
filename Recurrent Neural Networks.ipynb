{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# RNN using LSTM \n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN-rolled.png\"/ width=\"80px\" height=\"80px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN-unrolled.png\"/ width=\"400px\" height=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/LSTM3-chain.png\"/ width=\"800px\" height=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_source: http://colah.github.io/posts/2015-08-Understanding-LSTMs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.text import one_hot,text_to_word_sequence,base_filter\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n"
     ]
    }
   ],
   "source": [
    "DATA_DIRECTORY = os.path.join('data')\n",
    "print DATA_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIRECTORY,\"male_blog_list.txt\"),\"rb\") as male_file:\n",
    "    male_posts= pickle.load(male_file)\n",
    "with open(os.path.join(DATA_DIRECTORY,\"female_blog_list.txt\"),\"rb\") as female_file:\n",
    "    female_posts = pickle.load(female_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_male_posts = []\n",
    "filtered_female_posts = []\n",
    "\n",
    "for post_male in male_posts:\n",
    "    if len(post_male) == 0:\n",
    "        continue\n",
    "    filtered_male_posts.append(post_male)\n",
    "\n",
    "for post_female in female_posts:\n",
    "    if len(post_female) == 0:\n",
    "        continue\n",
    "    filtered_female_posts.append(post_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text processing - one hot builds index of the words\n",
    "male_one_hot = []\n",
    "female_one_hot = []\n",
    "n = 30000\n",
    "for post in filtered_male_posts:\n",
    "    try:\n",
    "        male_one_hot.append(one_hot(post,n,split=\" \",filters=base_filter(),lower=True))\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "for post in filtered_female_posts:\n",
    "    try:\n",
    "        female_one_hot.append(one_hot(post,n,split=\" \",filters=base_filter(),lower=True))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0 for male, 1 for female\n",
    "concatenate_array_rnn = np.concatenate((np.zeros(len(male_one_hot)),np.ones(len(female_one_hot))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_rnn,x_test_rnn,y_train_rnn,y_test_rnn = train_test_split(np.concatenate((female_one_hot,male_one_hot)),concatenate_array_rnn,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train_rnn shape:', (3706, 100), (3706,))\n",
      "('x_test_rnn shape:', (927, 100), (927,))\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "x_train_rnn = sequence.pad_sequences(x_train_rnn,maxlen=maxlen)\n",
    "x_test_rnn = sequence.pad_sequences(x_test_rnn,maxlen=maxlen)\n",
    "print('x_train_rnn shape:', x_train_rnn.shape,y_train_rnn.shape)\n",
    "print('x_test_rnn shape:', x_test_rnn.shape,y_test_rnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 128\n",
    "input_dimension = 128\n",
    "output_dimension = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dim = output_dimension, input_dim=input_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(input_dim=output_dimension, output_dim=1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3706 samples, validate on 927 samples\n",
      "Epoch 1/4\n",
      "3706/3706 [==============================] - 29s - loss: 0.2487 - val_loss: 0.2478\n",
      "Epoch 2/4\n",
      "3706/3706 [==============================] - 31s - loss: 0.2483 - val_loss: 0.2478\n",
      "Epoch 3/4\n",
      "3706/3706 [==============================] - 29s - loss: 0.2485 - val_loss: 0.2477\n",
      "Epoch 4/4\n",
      "3706/3706 [==============================] - 30s - loss: 0.2482 - val_loss: 0.2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nischal/anaconda2/lib/python2.7/site-packages/keras/models.py:385: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f43c7da90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_rnn,y_train_rnn,batch_size=32,nb_epoch=4,validation_data=(x_test_rnn,y_test_rnn),show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927/927 [==============================] - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24775136778977053"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_rnn,y_test_rnn,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_output = model.predict(x_test_rnn,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927/927 [==============================] - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test_rnn,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['predicted','actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print type(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47257459,  0.47150251,  0.47041783,  0.45502865,  0.46951264,\n",
       "        0.46091244,  0.47031957,  0.47972605,  0.46834189,  0.46798551,\n",
       "        0.47614822,  0.4678463 ,  0.47411162,  0.46905404,  0.46607849,\n",
       "        0.45298687,  0.46298698,  0.47241601,  0.48014697,  0.4752118 ,\n",
       "        0.47300324,  0.47477165,  0.44004026,  0.46681237,  0.44474387,\n",
       "        0.44107056,  0.47893703,  0.47737437,  0.46906811,  0.46242693,\n",
       "        0.47132847,  0.46405166,  0.48006848,  0.45297685,  0.4537138 ,\n",
       "        0.4697125 ,  0.46210769,  0.44221985,  0.45534465,  0.47295734,\n",
       "        0.45190829,  0.45475361,  0.45834431,  0.4756121 ,  0.45683247,\n",
       "        0.45522338,  0.45910046,  0.45680383,  0.47381619,  0.44358069,\n",
       "        0.45045453,  0.47051814,  0.47065336,  0.47720304,  0.46651417,\n",
       "        0.46838152,  0.46061912,  0.44572857,  0.46910205,  0.46858227,\n",
       "        0.46960133,  0.47160026,  0.47484818,  0.48020723,  0.47740191,\n",
       "        0.46048611,  0.4555622 ,  0.46555412,  0.48442936,  0.47129849,\n",
       "        0.46549663,  0.47621533,  0.46582165,  0.47333848,  0.46017027,\n",
       "        0.47142294,  0.4633902 ,  0.45640129,  0.45179683,  0.45047507,\n",
       "        0.45553795,  0.4673683 ,  0.47297934,  0.46630388,  0.45926848,\n",
       "        0.48164541,  0.48385468,  0.4704462 ,  0.44604301,  0.46462786,\n",
       "        0.46999049,  0.43623456,  0.46092978,  0.46678272,  0.46723667,\n",
       "        0.47306868,  0.46026275,  0.45909223,  0.46527007,  0.46627387,\n",
       "        0.45913818,  0.47450325,  0.47947463,  0.47317839,  0.45614246,\n",
       "        0.47562784,  0.46508506,  0.46883771,  0.44471326,  0.45949984,\n",
       "        0.45437425,  0.46727574,  0.46581519,  0.46392488,  0.45953405,\n",
       "        0.47984463,  0.44998512,  0.47330245,  0.44196507,  0.46460333,\n",
       "        0.45341566,  0.467071  ,  0.46668354,  0.44904074,  0.4624536 ,\n",
       "        0.46839625,  0.4635382 ,  0.48025858,  0.45692587,  0.43870789,\n",
       "        0.46410212,  0.47605282,  0.46911162,  0.47116101,  0.47000748,\n",
       "        0.48147255,  0.47955972,  0.47023457,  0.47368699,  0.45030719,\n",
       "        0.46913815,  0.46743336,  0.4762834 ,  0.47412059,  0.46959752,\n",
       "        0.45131481,  0.46770757,  0.47698027,  0.45563266,  0.47544247,\n",
       "        0.47246328,  0.45278275,  0.44044444,  0.46712247,  0.4639546 ,\n",
       "        0.47475335,  0.46927479,  0.46552807,  0.48525298,  0.46838367,\n",
       "        0.46636614,  0.4831093 ,  0.4660311 ,  0.46896699,  0.47245583,\n",
       "        0.47568002,  0.46097088,  0.46630695,  0.46043661,  0.4415994 ,\n",
       "        0.4709402 ,  0.47885191,  0.47425163,  0.46831861,  0.46306622,\n",
       "        0.47046104,  0.45990658,  0.45710161,  0.44652158,  0.46906212,\n",
       "        0.47265705,  0.44504404,  0.45278525,  0.4827978 ,  0.4448843 ,\n",
       "        0.45928282,  0.46650091,  0.48102114,  0.44425249,  0.44903952,\n",
       "        0.47683424,  0.47384843,  0.48195061,  0.46832341,  0.46175188,\n",
       "        0.48194122,  0.46663338,  0.46134767,  0.47528931,  0.45731595,\n",
       "        0.44901186,  0.45491093,  0.47947448,  0.46528763,  0.46715105,\n",
       "        0.47299603,  0.4801544 ,  0.46738631,  0.46412769,  0.46995297,\n",
       "        0.46153224,  0.47070888,  0.46399605,  0.44891438,  0.46501163,\n",
       "        0.44371867,  0.46047166,  0.45164546,  0.47206998,  0.47656894,\n",
       "        0.46764073,  0.47207585,  0.47385934,  0.4742018 ,  0.45489806,\n",
       "        0.4410111 ,  0.46088922,  0.45359966,  0.46093923,  0.46688107,\n",
       "        0.46976852,  0.46667388,  0.46777201,  0.47255731,  0.45658121,\n",
       "        0.4812786 ,  0.46444225,  0.46348187,  0.46184677,  0.47628164,\n",
       "        0.46645799,  0.47063896,  0.45904782,  0.48433578,  0.4399493 ,\n",
       "        0.47729298,  0.46150732,  0.47498864,  0.4762246 ,  0.47936624,\n",
       "        0.46714512,  0.47944957,  0.47104856,  0.46973041,  0.46949446,\n",
       "        0.47137403,  0.46693695,  0.4698976 ,  0.4503102 ,  0.48297366,\n",
       "        0.47783533,  0.46167558,  0.47526297,  0.46852618,  0.46360391,\n",
       "        0.46767077,  0.47830153,  0.45855665,  0.46727297,  0.46431476,\n",
       "        0.468768  ,  0.47267967,  0.46817273,  0.47178864,  0.47756812,\n",
       "        0.47378367,  0.47137931,  0.47928932,  0.48126486,  0.47510305,\n",
       "        0.47046325,  0.45512727,  0.46982604,  0.47919911,  0.45967603,\n",
       "        0.47038585,  0.46865621,  0.46925479,  0.47399682,  0.46087041,\n",
       "        0.47622815,  0.48157039,  0.46902987,  0.44679576,  0.46408868,\n",
       "        0.44334292,  0.47320548,  0.4728007 ,  0.46959034,  0.45486602,\n",
       "        0.46976292,  0.46217322,  0.48858756,  0.47842044,  0.47122577,\n",
       "        0.46127591,  0.45787916,  0.45822254,  0.48235935,  0.47576624,\n",
       "        0.47605172,  0.47016445,  0.47208184,  0.46676043,  0.46397573,\n",
       "        0.48024452,  0.47742662,  0.46696019,  0.47548354,  0.4636246 ,\n",
       "        0.46508211,  0.45142993,  0.4512012 ,  0.45814177,  0.44327819,\n",
       "        0.45517644,  0.45017412,  0.46379209,  0.46272662,  0.47228459,\n",
       "        0.4636153 ,  0.4777123 ,  0.47064072,  0.46988022,  0.46675056,\n",
       "        0.44630665,  0.47047412,  0.46039772,  0.46881956,  0.46238568,\n",
       "        0.45900372,  0.47284177,  0.4684577 ,  0.47406411,  0.47336712,\n",
       "        0.46712574,  0.44556639,  0.48484945,  0.47322384,  0.45878547,\n",
       "        0.48334488,  0.47163868,  0.46832687,  0.45950058,  0.47225115,\n",
       "        0.47131106,  0.47515643,  0.47081098,  0.46981588,  0.44161236,\n",
       "        0.4770202 ,  0.46894449,  0.47011149,  0.46849829,  0.4413594 ,\n",
       "        0.4734621 ,  0.47310519,  0.45429105,  0.4650121 ,  0.4803493 ,\n",
       "        0.46497083,  0.46179074,  0.46762389,  0.47330859,  0.46207744,\n",
       "        0.46677971,  0.46307415,  0.4850699 ,  0.46124929,  0.47001284,\n",
       "        0.47649375,  0.46940354,  0.47205088,  0.46512982,  0.4631103 ,\n",
       "        0.46118003,  0.43593866,  0.46959981,  0.46959755,  0.47071072,\n",
       "        0.46362886,  0.46658945,  0.46357265,  0.46711159,  0.459167  ,\n",
       "        0.46953505,  0.47386095,  0.47248593,  0.44386902,  0.47828108,\n",
       "        0.47885394,  0.47061029,  0.4457294 ,  0.47770104,  0.4406828 ,\n",
       "        0.46394554,  0.47507849,  0.44578987,  0.4531872 ,  0.47496873,\n",
       "        0.46765432,  0.46669796,  0.47299972,  0.45597091,  0.47551942,\n",
       "        0.44921568,  0.46967512,  0.47273198,  0.45963904,  0.46422213,\n",
       "        0.46540061,  0.47187328,  0.47959962,  0.47013551,  0.45633662,\n",
       "        0.46565485,  0.47265613,  0.47636384,  0.45801693,  0.47583231,\n",
       "        0.47350359,  0.46752089,  0.46812493,  0.47579917,  0.46696097,\n",
       "        0.43986568,  0.45483527,  0.47277045,  0.47459775,  0.43368345,\n",
       "        0.47426558,  0.46791887,  0.4631623 ,  0.46116358,  0.45113072,\n",
       "        0.46654022,  0.47154835,  0.46506709,  0.46866202,  0.44136664,\n",
       "        0.47299904,  0.45396277,  0.47334364,  0.4753024 ,  0.46746752,\n",
       "        0.47102019,  0.45606187,  0.47529238,  0.45416343,  0.47871047,\n",
       "        0.47569701,  0.45974559,  0.47621062,  0.44350964,  0.46894798,\n",
       "        0.46743214,  0.46590075,  0.47050321,  0.44643486,  0.47026408,\n",
       "        0.46719226,  0.46779943,  0.45451307,  0.47128364,  0.48103106,\n",
       "        0.44459134,  0.47217765,  0.46751681,  0.47497639,  0.45574859,\n",
       "        0.4592773 ,  0.46577191,  0.46935827,  0.46804085,  0.46988758,\n",
       "        0.48310214,  0.47555998,  0.46416533,  0.45610675,  0.46748823,\n",
       "        0.45507002,  0.47262195,  0.47713268,  0.46966693,  0.46466368,\n",
       "        0.47196898,  0.46273804,  0.45469582,  0.45616826,  0.47601599,\n",
       "        0.47348166,  0.46830037,  0.46905836,  0.46903437,  0.44845814,\n",
       "        0.46251887,  0.46635932,  0.45315567,  0.46370387,  0.46500948,\n",
       "        0.4676604 ,  0.47352856,  0.48278597,  0.45377341,  0.46768785,\n",
       "        0.46959028,  0.46629959,  0.47243452,  0.46673396,  0.48135984,\n",
       "        0.44758713,  0.47614464,  0.46007559,  0.46195275,  0.47750604,\n",
       "        0.46532699,  0.46867603,  0.4619132 ,  0.465884  ,  0.4787643 ,\n",
       "        0.46725234,  0.46381244,  0.46294266,  0.46999511,  0.47638857,\n",
       "        0.44497892,  0.46617469,  0.44502974,  0.44796011,  0.45413017,\n",
       "        0.4682377 ,  0.47201067,  0.47610062,  0.48116362,  0.47608823,\n",
       "        0.48256636,  0.45699799,  0.45525452,  0.47287792,  0.47552148,\n",
       "        0.47685507,  0.46465653,  0.47032148,  0.47593522,  0.46860275,\n",
       "        0.46392223,  0.44934633,  0.46670139,  0.47469121,  0.47626474,\n",
       "        0.47900653,  0.46788529,  0.48790655,  0.47227505,  0.44886661,\n",
       "        0.44244811,  0.48120019,  0.47791994,  0.46834108,  0.46705565,\n",
       "        0.47160885,  0.46782649,  0.46663418,  0.47702289,  0.4653708 ,\n",
       "        0.46480852,  0.46359658,  0.47868255,  0.44453105,  0.45381162,\n",
       "        0.47070724,  0.46948254,  0.44292155,  0.46687651,  0.47289756,\n",
       "        0.47005633,  0.44513854,  0.46375462,  0.4800275 ,  0.45621589,\n",
       "        0.46285859,  0.4596428 ,  0.45904866,  0.46940222,  0.44788682,\n",
       "        0.4661161 ,  0.46623668,  0.47973564,  0.47341585,  0.46927279,\n",
       "        0.47672251,  0.45078027,  0.46326372,  0.45932767,  0.46224895,\n",
       "        0.45556864,  0.44677097,  0.46430865,  0.46447545,  0.46875572,\n",
       "        0.46765384,  0.4688578 ,  0.45594269,  0.47202569,  0.46464613,\n",
       "        0.46088377,  0.47106093,  0.46022063,  0.47587293,  0.46934289,\n",
       "        0.45514223,  0.46404588,  0.46120423,  0.45306632,  0.46914536,\n",
       "        0.47784942,  0.4700703 ,  0.46027696,  0.46406567,  0.47023103,\n",
       "        0.44856408,  0.47082743,  0.47685027,  0.46035516,  0.4815211 ,\n",
       "        0.46704361,  0.46549311,  0.48257723,  0.46496892,  0.46102133,\n",
       "        0.46669406,  0.48329604,  0.47978362,  0.47149491,  0.47498786,\n",
       "        0.46215791,  0.4713735 ,  0.45591065,  0.46791175,  0.45907843,\n",
       "        0.45633972,  0.47076985,  0.45850804,  0.47628337,  0.46149412,\n",
       "        0.46477726,  0.47181499,  0.46709985,  0.46400809,  0.48824313,\n",
       "        0.47626516,  0.47312841,  0.45164955,  0.4590674 ,  0.47177064,\n",
       "        0.46996132,  0.46382627,  0.46604002,  0.44958809,  0.46462557,\n",
       "        0.47620821,  0.46639279,  0.46793902,  0.44434887,  0.47056559,\n",
       "        0.47349206,  0.47629306,  0.46285951,  0.47197741,  0.46348125,\n",
       "        0.47668019,  0.47706857,  0.45795929,  0.46547684,  0.46152359,\n",
       "        0.47656649,  0.47355655,  0.47173592,  0.47189167,  0.44858438,\n",
       "        0.45241663,  0.46774611,  0.46697235,  0.47379816,  0.47007489,\n",
       "        0.47840121,  0.47281983,  0.48021594,  0.48045862,  0.47135022,\n",
       "        0.47571415,  0.46762419,  0.46108848,  0.47477466,  0.45373461,\n",
       "        0.47408879,  0.45912328,  0.45732024,  0.46234396,  0.46567279,\n",
       "        0.47040007,  0.4748798 ,  0.46177259,  0.4726325 ,  0.45992988,\n",
       "        0.45391765,  0.46824199,  0.46501514,  0.48034897,  0.46592525,\n",
       "        0.47202346,  0.46868795,  0.44661054,  0.46890149,  0.46996358,\n",
       "        0.4614408 ,  0.46689293,  0.47223842,  0.45182458,  0.46842754,\n",
       "        0.46730646,  0.47176531,  0.47046605,  0.4686482 ,  0.48125005,\n",
       "        0.47154048,  0.46491134,  0.47257227,  0.46153399,  0.47361928,\n",
       "        0.47053397,  0.46033388,  0.47915453,  0.47016516,  0.44366086,\n",
       "        0.46893591,  0.47355595,  0.45152786,  0.46951309,  0.46791202,\n",
       "        0.47200176,  0.467347  ,  0.47793028,  0.46833977,  0.45413581,\n",
       "        0.44924045,  0.46418145,  0.46858069,  0.45825338,  0.46791023,\n",
       "        0.45715353,  0.4791041 ,  0.47639513,  0.46524364,  0.47478136,\n",
       "        0.45124921,  0.47249129,  0.4587656 ,  0.4827719 ,  0.4703306 ,\n",
       "        0.46153715,  0.46317351,  0.45665097,  0.44380376,  0.46251023,\n",
       "        0.46873394,  0.4775866 ,  0.46117973,  0.47615367,  0.46983621,\n",
       "        0.46642306,  0.45100984,  0.47411659,  0.46375954,  0.47201055,\n",
       "        0.48239055,  0.46323177,  0.46410465,  0.46595448,  0.46463087,\n",
       "        0.47287694,  0.47277334,  0.45134962,  0.4764387 ,  0.46546033,\n",
       "        0.44722146,  0.47296003,  0.47171551,  0.47269055,  0.47693634,\n",
       "        0.46047682,  0.44571939,  0.47462144,  0.446953  ,  0.44033054,\n",
       "        0.45510575,  0.48054093,  0.46294716,  0.47278416,  0.45066637,\n",
       "        0.48089129,  0.47814959,  0.47411454,  0.47162628,  0.48800772,\n",
       "        0.473764  ,  0.47478095,  0.46775246,  0.46946672,  0.4531171 ,\n",
       "        0.47964516,  0.45639932,  0.46026301,  0.47930592,  0.47409624,\n",
       "        0.47329116,  0.45968172,  0.46053448,  0.45711935,  0.4678171 ,\n",
       "        0.45970955,  0.4670743 ,  0.46523747,  0.459508  ,  0.43480429,\n",
       "        0.46836674,  0.46404913,  0.4752464 ,  0.47589612,  0.45905408,\n",
       "        0.45821363,  0.48248091,  0.45925719,  0.47869927,  0.46777925,\n",
       "        0.4681108 ,  0.47040644,  0.4548454 ,  0.47278872,  0.45705476,\n",
       "        0.45844018,  0.45471424,  0.46091494,  0.4666042 ,  0.47799218,\n",
       "        0.47071132,  0.46727046,  0.47332922,  0.47097588,  0.47268841,\n",
       "        0.46308106,  0.46816069,  0.47464043,  0.44393161,  0.47185063,\n",
       "        0.45964181,  0.45502505,  0.45541775,  0.44424132,  0.47076675,\n",
       "        0.47114512,  0.47219965,  0.46710548,  0.45111063,  0.44654149,\n",
       "        0.48158056,  0.46552381,  0.46800178,  0.46593916,  0.469973  ,\n",
       "        0.46863452,  0.47301638,  0.45477554,  0.46874428,  0.48272413,\n",
       "        0.46591932,  0.46833825,  0.44546422,  0.46492234,  0.48141158,\n",
       "        0.46430838,  0.4675869 ,  0.47684991,  0.47406039,  0.46212098,\n",
       "        0.48072946,  0.46110076,  0.45100945,  0.4419958 ,  0.46659246,\n",
       "        0.47374502,  0.47115791,  0.4633038 ,  0.45608708,  0.46634859,\n",
       "        0.47217458,  0.47062838,  0.47904211,  0.46085128,  0.46095747,\n",
       "        0.4709824 ,  0.4416998 ,  0.46790257,  0.48001364,  0.4757106 ,\n",
       "        0.47125676,  0.44494498,  0.47100085,  0.4754495 ,  0.47308379,\n",
       "        0.45002562,  0.45856729,  0.46597287,  0.46320882,  0.47091356,\n",
       "        0.45435038,  0.47304559])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['predicted'] = predicted_output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['actual'] = y_test_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.471503</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.455029</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469513</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predicted  actual\n",
       "0   0.472575       0\n",
       "1   0.471503       1\n",
       "2   0.470418       0\n",
       "3   0.455029       1\n",
       "4   0.469513       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TFIDF Vectorizer as an input instead of one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', norm='l2')\n",
    "tfidf_male = vectorizer.fit_transform(filtered_male_posts)\n",
    "tfidf_female = vectorizer.fit_transform(filtered_female_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened_array_tfidf_male = tfidf_male.toarray()\n",
    "flattened_array_tfidf_female = tfidf_male.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concatenate_array_rnn = np.concatenate((np.zeros(len(flattened_array_tfidf_male)),np.ones(len(flattened_array_tfidf_female))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_rnn,x_test_rnn,y_train_rnn,y_test_rnn = train_test_split(np.concatenate((flattened_array_tfidf_male,flattened_array_tfidf_female)),concatenate_array_rnn,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "# x_train_rnn = sequence.pad_sequences(x_train_rnn,maxlen=maxlen)\n",
    "# x_test_rnn = sequence.pad_sequences(x_test_rnn,maxlen=maxlen)\n",
    "# print('x_train_rnn shape:', x_train_rnn.shape,y_train_rnn.shape)\n",
    "# print('x_test_rnn shape:', x_test_rnn.shape,y_test_rnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "dimension = 128\n",
    "input_dimension = 128\n",
    "output_dimension = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dimension))\n",
    "model.add(LSTM(output_dim = output_dimension, input_dim=input_dimension))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(input_dim=output_dimension, output_dim=1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_rnn,y_train_rnn,batch_size=32,nb_epoch=4,validation_data=(x_test_rnn,y_test_rnn),show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(x_test_rnn,y_test_rnn,batch_size=32,show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation using RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading all the male text data into one string\n",
    "male_post = ' '.join(filtered_male_posts[:2])\n",
    "\n",
    "#building character set for the male posts\n",
    "character_set_male = set(male_post)\n",
    "#building two indices - character index and index of character\n",
    "char_indices = dict((c, i) for i, c in enumerate(character_set_male))\n",
    "indices_char = dict((i, c) for i, c in enumerate(character_set_male))\n",
    "\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 20\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(male_post) - maxlen, step):\n",
    "    sentences.append(male_post[i : i + maxlen])\n",
    "    next_chars.append(male_post[i + maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectorisation of input\n",
    "x_male = np.zeros((len(male_post),maxlen,len(character_set_male)),dtype=np.bool)\n",
    "y_male = np.zeros((len(male_post),len(character_set_male)),dtype=np.bool)\n",
    "\n",
    "print x_male.shape,y_male.shape\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_male[i, t, char_indices[char]] = 1\n",
    "    y_male[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print x_male.shape,y_male.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Building the model to generate text with 2 layers\n",
    "auto_text_generating_male_model = Sequential()\n",
    "auto_text_generating_male_model.add(LSTM(len(character_set_male),512,return_sequences=True))\n",
    "auto_text_generating_male_model.add(Dropout(0.2))\n",
    "auto_text_generating_male_model.add(LSTM(512,512,return_sequences=False))\n",
    "auto_text_generating_male_model.add(Dropout(0.2))\n",
    "auto_text_generating_male_model.add(Dense(512,len(character_set_male)))\n",
    "auto_text_generating_male_model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auto_text_generating_male_model.compile(loss='mean_squared_error',optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to sample an index from a probability array\n",
    "def sample(a, diversity=0.75):\n",
    "    if random.random() > diversity:\n",
    "        return np.argmax(a)\n",
    "    while 1:\n",
    "        i = random.randint(0, len(a)-1)\n",
    "        if a[i] > random.random():\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1,10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    auto_text_generating_male_model.fit(x_male, y_male, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(male_post) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.4, 0.6, 0.8]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = male_post[start_index : start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "        for iteration in range(400):\n",
    "            try:\n",
    "                x = np.zeros((1, maxlen, len(character_set_male)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = auto_text_generating_male_model.predict(x, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                #sys.stdout.write(next_char)\n",
    "                #sys.stdout.flush()\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        print sentence\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
